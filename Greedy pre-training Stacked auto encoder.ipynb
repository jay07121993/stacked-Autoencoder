{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import StratifiedKFold,KFold,ShuffleSplit\n",
    "import cntk as c\n",
    "import matplotlib.pyplot as plt\n",
    "from keras import backend as c\n",
    "from keras import layers\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.convolutional import Conv1D, MaxPooling1D\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras import regularizers\n",
    "from keras.layers import Input, Dense\n",
    "from keras import losses\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(144766, 40)\n",
      "(125973, 5)\n",
      "(18793, 5)\n"
     ]
    }
   ],
   "source": [
    "x_train= np.loadtxt('C:/Users/Devanshu/Desktop/faltu/data without misslabel/train_test_without_miss_normalize.csv', delimiter=',', dtype='float32')\n",
    "x_train_label= np.loadtxt('C:/Users/Devanshu/Desktop/faltu/data without misslabel/label_encoded_train.csv', delimiter=',', dtype='float32')\n",
    "x_test_label= np.loadtxt('C:/Users/Devanshu/Desktop/faltu/data without misslabel/label_encoded_test.csv', delimiter=',', dtype='float32')\n",
    "print(x_train.shape)\n",
    "print(x_train_label.shape)\n",
    "print(x_test_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(125973, 40) (18793, 40)\n"
     ]
    }
   ],
   "source": [
    "train= x_train[0:125973,:]\n",
    "test= x_train[125973:,:]\n",
    "train_label= x_train_label\n",
    "print(train.shape,test.shape)\n",
    "#print(train.shape, train_label.shape,test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 100778 samples, validate on 25195 samples\n",
      "Epoch 1/1\n",
      "100778/100778 [==============================] - 19s 191us/step - loss: 0.0065 - val_loss: 0.0021\n",
      "(125973, 30)\n"
     ]
    }
   ],
   "source": [
    "# layer 1\n",
    "input_ly_1= Input(shape=(40,))\n",
    "enc_ly_1= Dense(30, activation='relu')(input_ly_1)\n",
    "dec_ly_1=Dense(40, activation='relu')(enc_ly_1)\n",
    "\n",
    "auto_ly_1= Model(input_ly_1, dec_ly_1)\n",
    "encoder_ly_1= Model(input_ly_1, enc_ly_1)\n",
    "\n",
    "auto_ly_1.compile(optimizer='adam', loss='mean_squared_error')\n",
    "auto_ly_1.fit(train,train,epochs=1,validation_split=0.2)\n",
    "\n",
    "\n",
    "ly1_predict= encoder.predict(train)\n",
    "print(ly1_predict.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 100778 samples, validate on 25195 samples\n",
      "Epoch 1/1\n",
      "100778/100778 [==============================] - 18s 176us/step - loss: 0.0956 - val_loss: 0.0869\n",
      "(125973, 15)\n"
     ]
    }
   ],
   "source": [
    "#layer 2\n",
    "input_ly_2= Input(shape=(30,))\n",
    "enc_ly_2= Dense(15, activation='relu')(input_ly_2)\n",
    "dec_ly_2= Dense(30, activation='relu')(enc_ly_2)\n",
    "\n",
    "auto_ly_2= Model(input_ly_2, dec_ly_2)\n",
    "encoder_ly_2= Model(input_ly_2, enc_ly_2)\n",
    "\n",
    "auto_ly_2.compile(optimizer='adam', loss='mean_squared_error')\n",
    "auto_ly_2.fit(ly1_predict,ly1_predict,epochs=1,validation_split=0.2)\n",
    "\n",
    "ly2_predict= encoder_ly_2.predict(ly1_predict)\n",
    "print(ly2_predict.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 100778 samples, validate on 25195 samples\n",
      "Epoch 1/1\n",
      "100778/100778 [==============================] - 17s 167us/step - loss: 0.3416 - val_loss: 0.3052\n",
      "(125973, 5)\n"
     ]
    }
   ],
   "source": [
    "#layer 3\n",
    "input_ly_3= Input(shape=(15,))\n",
    "enc_ly_3= Dense(5, activation='relu')(input_ly_3)\n",
    "dec_ly_3= Dense(15, activation='relu')(enc_ly_3)\n",
    "\n",
    "auto_ly_3= Model(input_ly_3, dec_ly_3)\n",
    "encoder_ly_3= Model(input_ly_3, enc_ly_3)\n",
    "\n",
    "auto_ly_3.compile(optimizer='adam', loss='mean_squared_error')\n",
    "auto_ly_3.fit(ly2_predict,ly2_predict,epochs=1,validation_split=0.2)\n",
    "\n",
    "ly3_predict= encoder_ly_3.predict(ly2_predict)\n",
    "print(ly3_predict.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 100778 samples, validate on 25195 samples\n",
      "Epoch 1/10\n",
      "100778/100778 [==============================] - 11s 108us/step - loss: 0.0447 - acc: 0.3005 - val_loss: 0.0262 - val_acc: 0.5394\n",
      "Epoch 2/10\n",
      "100778/100778 [==============================] - 10s 98us/step - loss: 0.0229 - acc: 0.3861 - val_loss: 0.0208 - val_acc: 0.3906\n",
      "Epoch 3/10\n",
      "100778/100778 [==============================] - 11s 105us/step - loss: 0.0200 - acc: 0.2864 - val_loss: 0.0194 - val_acc: 0.2053\n",
      "Epoch 4/10\n",
      "100778/100778 [==============================] - 10s 96us/step - loss: 0.0190 - acc: 0.2812 - val_loss: 0.0188 - val_acc: 0.3286\n",
      "Epoch 5/10\n",
      "100778/100778 [==============================] - 10s 102us/step - loss: 0.0186 - acc: 0.2760 - val_loss: 0.0185 - val_acc: 0.2897\n",
      "Epoch 6/10\n",
      "100778/100778 [==============================] - 10s 102us/step - loss: 0.0184 - acc: 0.2785 - val_loss: 0.0184 - val_acc: 0.1611\n",
      "Epoch 7/10\n",
      "100778/100778 [==============================] - 11s 108us/step - loss: 0.0183 - acc: 0.2815 - val_loss: 0.0182 - val_acc: 0.2415\n",
      "Epoch 8/10\n",
      "100778/100778 [==============================] - 12s 120us/step - loss: 0.0182 - acc: 0.2729 - val_loss: 0.0181 - val_acc: 0.3018\n",
      "Epoch 9/10\n",
      "100778/100778 [==============================] - 14s 138us/step - loss: 0.0181 - acc: 0.2727 - val_loss: 0.0181 - val_acc: 0.1572\n",
      "Epoch 10/10\n",
      "100778/100778 [==============================] - 13s 128us/step - loss: 0.0180 - acc: 0.2687 - val_loss: 0.0180 - val_acc: 0.2848\n",
      "Train on 100778 samples, validate on 25195 samples\n",
      "Epoch 1/10\n",
      "100778/100778 [==============================] - 9s 91us/step - loss: 0.0251 - acc: 0.9246 - val_loss: 0.0121 - val_acc: 0.9657\n",
      "Epoch 2/10\n",
      "100778/100778 [==============================] - 9s 90us/step - loss: 0.0107 - acc: 0.9696 - val_loss: 0.0094 - val_acc: 0.9732\n",
      "Epoch 3/10\n",
      "100778/100778 [==============================] - 10s 103us/step - loss: 0.0089 - acc: 0.9733 - val_loss: 0.0081 - val_acc: 0.9777\n",
      "Epoch 4/10\n",
      "100778/100778 [==============================] - 10s 96us/step - loss: 0.0072 - acc: 0.9770 - val_loss: 0.0063 - val_acc: 0.9808\n",
      "Epoch 5/10\n",
      "100778/100778 [==============================] - 7s 71us/step - loss: 0.0059 - acc: 0.9825 - val_loss: 0.0056 - val_acc: 0.9842\n",
      "Epoch 6/10\n",
      "100778/100778 [==============================] - 6s 62us/step - loss: 0.0053 - acc: 0.9847 - val_loss: 0.0049 - val_acc: 0.9858\n",
      "Epoch 7/10\n",
      "100778/100778 [==============================] - 7s 68us/step - loss: 0.0048 - acc: 0.9861 - val_loss: 0.0046 - val_acc: 0.9858\n",
      "Epoch 8/10\n",
      "100778/100778 [==============================] - 8s 78us/step - loss: 0.0045 - acc: 0.9869 - val_loss: 0.0044 - val_acc: 0.9879\n",
      "Epoch 9/10\n",
      "100778/100778 [==============================] - 7s 74us/step - loss: 0.0042 - acc: 0.9878 - val_loss: 0.0041 - val_acc: 0.9885\n",
      "Epoch 10/10\n",
      "100778/100778 [==============================] - 7s 65us/step - loss: 0.0039 - acc: 0.9888 - val_loss: 0.0038 - val_acc: 0.9896\n",
      "18793/18793 [==============================] - 1s 59us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.0836193269437912, 0.7656042143446397]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Deep stacked auto encoder\n",
    "ly_30= auto_ly_1.layers[1].get_weights()\n",
    "ly_15= auto_ly_2.layers[1].get_weights()\n",
    "ly_5= auto_ly_3.layers[1].get_weights()\n",
    "\n",
    "\n",
    "input_da= Input(shape=(40,))\n",
    "da_1=Dense(30, activation='relu', input_shape=(40,), name=\"layer1\")(input_da)\n",
    "da_2= Dense(15, activation='relu',name=\"layer2\")(da_1)\n",
    "da_3= Dense(5, activation='softmax',)(da_2)\n",
    "da_4= Dense(15, activation='relu',)(da_3)\n",
    "da_5= Dense(30, activation='relu',)(da_4)\n",
    "da_6= Dense(40, activation='relu',)(da_5)\n",
    "\n",
    "da= Model(input_da, da_6)\n",
    "da_2= Model(input_da, da_3)\n",
    "\n",
    "da.layers[1].set_weights(ly_30) # first dense layer\n",
    "da.layers[2].set_weights(ly_15)\n",
    "da.layers[3].set_weights(ly_5)\n",
    "\n",
    "da.compile(optimizer='adam', loss='mean_squared_error', metrics=['accuracy'])\n",
    "da.fit(train, train,epochs=10,batch_size=128,shuffle=True,verbose=1, validation_split=0.2)\n",
    "\n",
    "da_2.compile(optimizer='adam', loss='mean_squared_error', metrics=['accuracy'])\n",
    "da_2.fit(train, train_label,epochs=10,batch_size=128,shuffle=True,verbose=1, validation_split=0.2)\n",
    "da_2.evaluate(test,x_test_label)\n",
    "#sdg= da_2.predict(train)\n",
    "#np.savetxt('C:/Users/Devanshu/Desktop/faltu/predict_layer_autostack_train.csv', sdg, delimiter=',')\n",
    "\n",
    "#deep_autoencoder.layers[1].set_weights(autoencoder1.layers[2].get_weights()) # first dense layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Just for reference. Code below is not for use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model2= Sequential()\n",
    "model2.add(Dense(15, activation='relu', input_shape=(30,), name=\"layer2\"))\n",
    "ly= model1.layers[0].get_weights()\n",
    "#model2.add(Dense(15, activation='relu', input_shape=(30,), name=\"layer2\"))\n",
    "model2.add(Dense(30, activation='relu'))\n",
    "\n",
    "model2.compile(optimizer='adam', loss='mean_squared_error')\n",
    "model2.fit(train, train,epochs=5,batch_size=128,shuffle=True,verbose=1, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 100778 samples, validate on 25195 samples\n",
      "Epoch 1/5\n",
      "100778/100778 [==============================] - 8s 82us/step - loss: 0.0495 - val_loss: 0.0314\n",
      "Epoch 2/5\n",
      "100778/100778 [==============================] - 7s 69us/step - loss: 0.0289 - val_loss: 0.0272\n",
      "Epoch 3/5\n",
      "100778/100778 [==============================] - 7s 70us/step - loss: 0.0264 - val_loss: 0.0257\n",
      "Epoch 4/5\n",
      "100778/100778 [==============================] - 7s 69us/step - loss: 0.0253 - val_loss: 0.0249\n",
      "Epoch 5/5\n",
      "100778/100778 [==============================] - 7s 72us/step - loss: 0.0247 - val_loss: 0.0244\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x14485672048>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1= Sequential()\n",
    "input_img = Input(shape=(40,))\n",
    "model1.add(Dense(30, activation='relu', input_shape=(40,), name=\"layer1\"))\n",
    "ly= model1.layers[0].get_weights()\n",
    "model1.add(Dense(15, activation='relu',name=\"layer2\",trainable=False))\n",
    "model1.add(Dense(5, activation='relu',trainable=False))\n",
    "\n",
    "model1.compile(optimizer='adam', loss='mean_squared_error')\n",
    "model1.fit(train, train,epochs=5,batch_size=128,shuffle=True,verbose=1, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 100778 samples, validate on 25195 samples\n",
      "Epoch 1/10\n",
      "100778/100778 [==============================] - 7s 66us/step - loss: 0.0275 - acc: 0.9118 - val_loss: 0.0109 - val_acc: 0.9692\n",
      "Epoch 2/10\n",
      "100778/100778 [==============================] - 7s 66us/step - loss: 0.0097 - acc: 0.9723 - val_loss: 0.0088 - val_acc: 0.9758\n",
      "Epoch 3/10\n",
      "100778/100778 [==============================] - 7s 67us/step - loss: 0.0082 - acc: 0.9756 - val_loss: 0.0074 - val_acc: 0.9775\n",
      "Epoch 4/10\n",
      "100778/100778 [==============================] - 7s 69us/step - loss: 0.0072 - acc: 0.9798 - val_loss: 0.0066 - val_acc: 0.9819\n",
      "Epoch 5/10\n",
      "100778/100778 [==============================] - 7s 66us/step - loss: 0.0066 - acc: 0.9820 - val_loss: 0.0064 - val_acc: 0.9828\n",
      "Epoch 6/10\n",
      "100778/100778 [==============================] - 7s 65us/step - loss: 0.0063 - acc: 0.9827 - val_loss: 0.0060 - val_acc: 0.9835\n",
      "Epoch 7/10\n",
      "100778/100778 [==============================] - 7s 66us/step - loss: 0.0061 - acc: 0.9833 - val_loss: 0.0059 - val_acc: 0.9838\n",
      "Epoch 8/10\n",
      "100778/100778 [==============================] - 7s 66us/step - loss: 0.0059 - acc: 0.9833 - val_loss: 0.0057 - val_acc: 0.9840\n",
      "Epoch 9/10\n",
      "100778/100778 [==============================] - 7s 71us/step - loss: 0.0057 - acc: 0.9838 - val_loss: 0.0056 - val_acc: 0.9842\n",
      "Epoch 10/10\n",
      "100778/100778 [==============================] - 7s 71us/step - loss: 0.0055 - acc: 0.9839 - val_loss: 0.0055 - val_acc: 0.9844\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x144856c0e48>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2= Sequential()\n",
    "input_img = Input(shape=(40,))\n",
    "model2.add(Dense(30, activation='relu', input_shape=(40,), name=\"layer1\"))\n",
    "ly= model2.layers[0].get_weights()\n",
    "model2.add(Dense(15, activation='relu',name=\"layer2\",trainable=False))\n",
    "model2.add(Dense(5, activation='relu',trainable=False))\n",
    "\n",
    "model2.compile(optimizer='adam', loss='mean_squared_error')\n",
    "model.fit(train, train,epochs=5,batch_size=128,shuffle=True,verbose=1, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model3= Sequential()\n",
    "input_img = Input(shape=(40,))\n",
    "model3.add(Dense(30, activation='relu', input_shape=(40,), name=\"layer1\"))\n",
    "ly= model1.layers[0].get_weights()\n",
    "model1.add(Dense(15, activation='relu',name=\"layer2\",trainable=False))\n",
    "model1.add(Dense(5, activation='relu',trainable=False))\n",
    "\n",
    "model1.compile(optimizer='adam', loss='mean_squared_error')\n",
    "model1.fit(train, train,epochs=5,batch_size=128,shuffle=True,verbose=1, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
